\section{Approach}\label{sec:approach}

\subsection{Scope of the work}

In this work, it is intended to reproduce the method proposed by the original Netflix attack. Hence, \autoref{alg:algo} has been implemented in Python. To evaluate the performance of the two different scoring metrics that have been introduced, both have been implemented and they will be compared later.

However, several differences with the original paper are to be highlighted. First, it used 50 records from the IMDb. However, the only datasets that are currently publicly available are the ratings for each movie, but not the ratings from a given user. A solution would be to get this data directly from the IMDb website because the ratings of a user are public if he also wrote a review. A data miner that parses the content of random IMDb users could do the job, but there are two obstacles:

\begin{itemize}
	\item it would be of significant complexity, and is out of scope of the project.
	\item the terms and conditions of IMDb prohibit the usage of "data mining, robots, screen scraping, or similar data gathering and extraction tools‚Äù\footnote{see \url{https://www.imdb.com/conditions}}. It can be suspected that it is the reason only 50 entries we used in the original attack. 
\end{itemize}

As a workaround, we propose to use the MovieLens dataset as auxiliary information. MovieLens is a web-based movie recommender system that makes its database available for research. This database has already been used in privacy-related research, such as \cite{movielens}. As opposed to the IMDb case, the "anonymous" user IDs are consistent across all the movie ratings that are registered, which makes it suitable for the user re-identification. Also, it is the occasion to test another dataset against the Netflix one.

Finally, due to the significant sizes of both the Netflix and MovieLens datasets (100 and 20 millions entries), it was necessary to subsample them. Results are expected to suffer from the reduced number of samples.

\subsection{Data pre-processing}

Here: data presentation (numbers + structure), necessary processing before actual similarity computation