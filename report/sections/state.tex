\section{State of the art}\label{sec:state}

\subsection{De-anonymization attacks}

One of the most mentioned de-anonymization deeds dates back to 2006, when The New York Times journalists identified Thelma Arnold in the "anonymized" search queries released by AOL for research purposes \cite{nytimes}. By manually searching in the 20 million search queries coming from 657,000 users, the reporters could tie Arnold's identity to some quite embarrassing queries.

The computer-aided attacks, being able to push such results to a much larger scale, can be separated in several categories depending on their approach \cite{survey}. The two most represented methods are:

\begin{itemize}
	\item \textbf{Graph matching.} It is the most common approach in the case of social network de-anonymisation studies and is based on social graphs. One meaningful example is in \cite{graph_twitter}, where Flicker and Twitter accounts were linked together with a 12\% error rate. Several complex strategies can be used to improve graph matching, such as Seed \& Grow \cite{seed} or Threading \cite{threading}.
	
	\item \textbf{Similarity matching.} It is based on similar features between the target and auxiliary information. In \cite{tweets}, users were de-anonymised using the similarity between tweets and the content of their resume. In \cite{homicide}, victims of homicides were re-identified using "anonymous" homicides public records of Chicago and records in the Social Security Death Index.
\end{itemize}

\subsection{The Netflix case}

The attack that will be reproduced is the one presented in \cite{netflix}, an example of similarity matching. In this paper, researchers attacked a dataset released by Netflix in the context of a contest to improve their recommendation system. The 100 millions movie ratings by over 480,000 users were correlated to another movie rating database: the Internet Movie Database (IMDb). The goal was to link together private Netflix accounts and public IMDb accounts based on the published ratings. In a very small sample of the IMDb (50 users only), 2 users of the Netflix dataset were identified with statistical quasi-certainty. As the authors summarized, given a few of an user's reviews that he chose to make \textit{public}, their algorithm is able to access all of his \textit{private} Netflix ratings. 

The algorithm is based on the idea of a similarity measure denoted $Sim$. It is introduced, for two records $r_1$ and $r_2$, with $supp$ denoting the non-null attributes of a record:

\begin{equation}
	Sim(r_1, r_2) = \frac{\sum Sim_{\text{cos}}(r_{1i}, r_{2i})}{\lvert supp(r_1) \cup supp(r_2) \rvert}
\end{equation} 

With $Sim_{\text{cos}}$ denoting the cosine similarity measure, the function $Sim$ maps the records $r_1$ and $r_2$ to an interval $[0,1]$, representing the notion of them being similar. This concept now needs to be adapted to the specific content of a movie review dataset. In particular, the scoring function needs to give higher importance to statistically rare attributes. Indeed, a review on "The Longest Most Meaningless Movie in the World\footnote{See its Wikipedia page: \url{https://en.wikipedia.org/wiki/The_Longest_Most_Meaningless_Movie_in_the_World}}" helps to uniquely identify a user much more than the knowledge of the fact that he liked the last episode of "Game of Thrones". Also, the two pieces of information that are available for a given review are the score given by the user and the timestamp of the rating. The final scoring function that was used in \cite{netflix} is:

\begin{equation}\label{eq:score}
	Score(r,aux) = \sum_{i \in supp(aux)} \frac{1}{\log\lvert supp(i) \rvert} \left( e^{-\frac{ \lvert \rho_i - \rho_i'  \rvert}{\rho_0}} + e^{-\frac{\lvert d_i - d_i' \rvert}{d_0}}\right)
\end{equation}

In the $Score$ function that compares a record $r$ and auxiliary information $aux$, $\rho$ and $\rho'$ denote the rating given to the same movie, while $d$ and $d'$ refer to the date of the rating. $\rho_0$ and $d_0$ are constants empirically determined to respectively 1.5 and 30. The closest the ratings and the timestamps are, the higher the scoring function will be.

The fact whether a match has been found does not rely solely on the search for the entry in $aux$ that has the highest score. Indeed, this only indicates which entry is the most similar but does not take into account how strong is the similarity. This is managed by imposing that two entries from $r$ and $aux$ are considered a match only if the difference between the best and second-best scores is higher than a threshold, referred to as the eccentricity $\phi$. Mathematically, it is defined as:

\begin{equation}
	\phi = \frac{S_1-S_2}{\sigma_S}
\end{equation}

Where $S_1$ and $S_2$ denote respectively the best and second best score for a record $r$, and $\sigma_S$ denotes the standard deviation of all the scores related to the record $r$. It was proposed in \cite{netflix} that a match is considered to be found if $\phi > 1.5$. It is worth noting that the two matches with the 50 samples from IMDb had an eccentricity of respectively 28 and 15. These especially high numbers lead to the belief that two matches were found with statistical quasi-certainty.

In \cite{netflix-analytic}, theorems that formally demonstrate why the scoring expressed by \autoref{eq:score} works on the Netflix dataset were introduced. In addition to providing a theoretical framework, they propose another scoring algorithm, slightly less performing but based on more general assumptions.

In both \cite{netflix} and \cite{netflix-analytic}, the matching algorithm is the same (only the metrics that are used differ) and it is summarized in \autoref{alg:algo}.

\begin{algorithm}[h]
	\caption{Matching algorithm based on weighted scale scoring.}
	\label{alg:algo}
	\begin{algorithmic}[1]
		\State Starting from datasets $R$ and $aux$
		\newline
		\For {each record $r_i$ in $R$}
			\For {each entry $aux_i$ in $aux$}
				\State Compute $Score(r_i,aux_i)$
			\EndFor
			\State Compute $\sigma_S$ = \texttt{stdev(}$Score\texttt{)}$
			\State Find $S_1 = \max(Score(r_i, aux))$
			\State Find $S_2 = \max(Score(r_i, aux) \textbackslash \{S_1\})$
			\State Compute $\phi = (S_1-S_2)/\sigma_S$
			\If {$\phi$ > 1.5}
				\State Match found !
			\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}


